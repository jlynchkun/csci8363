#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
CSci 8363 Final Project Paper
\end_layout

\begin_layout Author
Thomas Christie
\end_layout

\begin_layout Author
Joshua Lynch
\end_layout

\begin_layout Part
Introduction 
\end_layout

\begin_layout Section
Motivation
\end_layout

\begin_layout Standard
The availability of detailed bioinformatic data is growing quickly, but
 reliable methods of relating this wealth of information to patient prognosis
 and diagnosis are often lacking.
 In the medical domain, biometric data often consists of an array of features
 for each patient.
 These features may be simple measures such as height and weight, or the
 results of complex analyses such as the presence or absence of certain
 genetic sequences in a patient's DNA.
 
\end_layout

\begin_layout Standard
With the recent explosive growth in computing power, it is becoming more
 common to collect all available data and determine relationships between
 data values and patient prognosis and outcomes after the fact.
 For example, recent studies have shown that collected gene expression data
 (
\begin_inset CommandInset citation
LatexCommand cite
key "deMJ:TheNewEnglandJournalOfMedicine:2002"

\end_inset

) and quantatized features from breast cancer images (
\begin_inset CommandInset citation
LatexCommand cite
key "Beck:SciTranslMed:2011"

\end_inset

) are predictive of cancer metastasis.
 Though this 
\begin_inset Quotes eld
\end_inset

data-driven
\begin_inset Quotes erd
\end_inset

 analysis can lead to the production of unnecessarily large amounts of data,
 there are many bnefits to this approach.
 First, data is not deemed useless apriori due to the verdict of domain
 experts, and this can lead to the discovery of previously unexplored relationsh
ips.
 In (IMAGE STUDY), image features were discovered that predicted patient
 outcome better than features traditionally used by physicians (BE MORE
 SPECIFIC).
 Moreover, relationships between features can increase the predictive power
 of the dataset.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang:Bioinformatics:2011"

\end_inset

, relationships between gene expression data and ??? were ???.
\end_layout

\begin_layout Section
Mathematical Approach
\end_layout

\begin_layout Standard
In the case of biometric data, it can be useful to assume that an underlying
 
\begin_inset Quotes eld
\end_inset

cause
\begin_inset Quotes erd
\end_inset

 is expressed in several of the collected features.
 For example, the presence of a specific gene can give rise to the synthesis
 of many proteins (SOURCE?).
 Similarly, a disease may give rise to a collection of phenotypes.
 One way to model this relationship is to posit a latent causal or explanatory
 structure underlying the measurable data (see FIGURE ?).
 Discovering hidden explanatory features can potentially lead to a simple
 explanation for various features, as well as drastically reduce the dimensional
ity of the data.
 Though the domain is drastically different, a simliar approach is often
 used to uncover semantic cotent underlying language data (see e.g.
 SOURCE?).
 
\end_layout

\begin_layout Standard
also relationships between different data types
\end_layout

\begin_layout Subsection
Matrix Decomposition
\end_layout

\begin_layout Standard
To model the relationship between latent variables and measurable features,
 we can assume that each feature is a linear combination of some number
 of latent variables.
 Mathematically, the features measured for each patient can be written as
 a vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 , where
\begin_inset Formula $\mathbf{X}=[x_{1}x_{2}...x_{n}]^{T}$
\end_inset

 is an 
\begin_inset Formula $m\times n$
\end_inset

 matrix with 
\begin_inset Formula $n$
\end_inset

 features and 
\begin_inset Formula $m$
\end_inset

 patients.
 In order to uncover the latent factors, we can decompose 
\begin_inset Formula $X$
\end_inset

 into two factors 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $H$
\end_inset

 such that 
\begin_inset Formula $X\approx WH$
\end_inset

.
 In this factorization, 
\begin_inset Formula $W$
\end_inset

 is 
\begin_inset Formula $m\times k$
\end_inset

 matrix whose component vectors span the column space of X, where entries
 of H provide vector weights.
 In other words, X consists of a linear combination of the column vectors
 of W with weights from H.
 W provides the 
\begin_inset Formula $k$
\end_inset

 
\begin_inset Quotes eld
\end_inset

sources
\begin_inset Quotes erd
\end_inset

 or latent variables and H describes how they linearly combine to produce
 the features in X.
 WH is an approximation of X, and the quality of the approximation is determined
 by partially by the choice of 
\begin_inset Formula $k$
\end_inset

.
 The dimension 
\begin_inset Formula $k$
\end_inset

 is often chosen heuristically, and different values of 
\begin_inset Formula $k$
\end_inset

 prove useful in different problem domains.
\end_layout

\begin_layout Standard
Matrix factorization is not unique, so an important question is how to decompose
 X into H and W.
 One principled way to perform such a factorization is to find component
 vectors that are mutually orthogonal.
 This is called Singular Value Decomposition (SVD), and constructs a unique
 factorization 
\begin_inset Formula $X=USV^{-1}$
\end_inset

, where U corresponds to W and the latter product corresponds to H (IS THIS
 TRUE? IS THERE NON-UNIQUE ORTHOGONAL MATRIX BI-FACTORIZATION?).
 This approach is mathematically robust and well-understood.
 A lower-rank approximation of X can be constructed using the first k columns
 of U, S and V-1.
 From the perspective of uncovering latient sources, however, SVD presents
 several problems.
 First, it may well be the case that latent sources are not actually orthogonal.
 In this case, the U matrix would not correspond to any domain-specific
 functional process but would be a mere mathematical convenience (CITE Josh's
 thing that he showed in class).
 In addition, SVD does not impose a constraint on the sign of values of
 H.
 A combination of positive and negative coeffients create both additive
 and subtractive components of each feature, making it very difficult to
 interpret how latent sources combine to create features.
 Lastly, SVD produces results which are dense (few 0s in the components
 of W and H).
 This implies that every latent source has an effect on every feature present
 in X, an implication which is likely not true in many applications.
 Approaches for addressing mixed signs, orthogonality and density are addressed
 below.
\end_layout

\begin_layout Subsection
Non-Negative Matrix Factorization (NMF)
\end_layout

\begin_layout Standard
One approach to creating easily interpretable latent sources is to introduce
 the additional requirement that the data have values greater than or equal
 to zero and perform matrix decomposition in such a way that values of H
 and W are exclusively nonnegative.
 This means that the underlying components of X combine in a purely additive
 way to produce X, which gives a more intuitive decomposition of the data
 matrix.
 Moreover, in situations where data is naturally nonnegative (such as image
 data or gene expression(?)), negative components may not be readily interpretab
le.
 
\end_layout

\begin_layout Standard
Biometric data often originates with mixed signs.
 When faced with a 
\begin_inset Formula $m\times n$
\end_inset

 matrix X with mixed-sign entries, a common technique for imposing nonnegativity
 is to construct a new 
\begin_inset Formula $m\times2n$
\end_inset

 matrix X'=[max(X,0) max(-X,0)].
 In other words, the left half of X' consists of the originally positive
 values in X with the negative values replaces by 0.
 The right half of X consists of the absolute value of originally negative
 values of X, with 0's replacing corresponding positive values.
 This is the approached used in the methods below.
\end_layout

\begin_layout Standard
Placing an orthogonality constraint on the vectors of the basis matrix W
 forces components to be uncorrelated.
 While this is useful from a data compression standpoint and has other convenien
t mathematical properties, it may well be the case that distinct underlying
 sources have overlapping manifestations in feature space.
 In the approaches used in this paper, the orthogonality constraint is dropped.
\end_layout

\begin_layout Subsection
Sparse Decomposition
\end_layout

\begin_layout Standard
(SEE KIM AND PARK 2007) A matrix is called sparse when the majority of its
 elements are 0.
 In cases where a sparse H can be found (W TOO OR JUST H?), the product
 WH can be seen as an approximation of X in which each feature of X is composed
 of a small number of columns of W.
 This is useful, as it highlights which latent components are most responsible
 for each feature.
 
\end_layout

\begin_layout Standard
Note that with sparse data, even through the orthogonality constraint is
 relaxed, columns have a high probability of being orthogonal.
 
\end_layout

\begin_layout Subsection
Matrix Decomposition as Optimization (JOSH??!)
\end_layout

\begin_layout Standard
Finding an approximation WH of the nxm matrix X can be construed as an optimizat
ion problem:
\begin_inset Formula 
\[
\min_{W,H}f(W,H)=\frac{1}{2}||X-WH||_{F}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $||\cdot||_{F}$
\end_inset

is the Frobenius norm, 
\begin_inset Formula $W\epsilon\mathbb{R}^{n\times k}$
\end_inset

 and 
\begin_inset Formula $H\epsilon\mathbb{R}^{k\times m}$
\end_inset

.
 Non-negativity can be imposed as an additional constraint on W and H.
 Sparsity is controlled by adding a term penalizing the size of the L0 norm
 of W and/or H.
 In practice the L1 norm is often used as an approximation of the L0 norm,
 as the L1 has more convenient mathematical properties (see section ???).
 
\end_layout

\begin_layout Subsubsection
Convex Optimization
\end_layout

\begin_layout Standard
how it applies to this problem
\end_layout

\begin_layout Subsubsection
Legrange Multipliers
\end_layout

\begin_layout Subsubsection
Multiplicative Updates (THOMAS?)
\end_layout

\begin_layout Standard
show how to re-write equation as multiplicative update
\end_layout

\begin_layout Part
Experiments
\end_layout

\begin_layout Standard
Goals: 
\end_layout

\begin_layout Standard
1.
 use decomposition to predict metastasis (cool, but can really do this with
 other nonlinear methods).
 
\end_layout

\begin_layout Standard
2.
 Uncover relationships between features.
 This is similar to finding underlying 
\begin_inset Quotes eld
\end_inset

latent
\begin_inset Quotes erd
\end_inset

 sources that combine to create the features, i.e.
 
\begin_inset Quotes eld
\end_inset

co-clustering
\begin_inset Quotes erd
\end_inset

.
 
\end_layout

\begin_layout Standard
3.
 If you have two different types of data, figure out how they relate to
 each other.
 This overlaps with part (2) somewhat, but it depends on how you formulate
 the problem.
\end_layout

\begin_layout Standard
Can do both of these things this in both a supervised (using labels) and
 unsupervised).
\end_layout

\begin_layout Section
Supervised Approach
\end_layout

\begin_layout Standard
-find relationships between two datasets
\end_layout

\begin_layout Standard
find underlying sources of features as they relate to some label (patient
 outcome) (like LSI)
\end_layout

\begin_layout Standard
constrain this using label as one of the factors
\end_layout

\begin_layout Standard
-non-negative, sparse
\end_layout

\begin_layout Subsection
Non-negative Matrix Tri-factorization
\end_layout

\begin_layout Subsection
Problem Formulation (incl multiplicative updates)
\end_layout

\begin_layout Subsection
Results	
\end_layout

\begin_layout Section
Unsupervised Approach
\end_layout

\begin_layout Standard
find latent features that predict patient outcome
\end_layout

\begin_layout Standard
optional: find relationship between two datasets
\end_layout

\begin_layout Standard
-non-negative, sparse
\end_layout

\begin_layout Subsection
Bifactorization with additional constraints
\end_layout

\begin_layout Subsection
Comodules??
\end_layout

\begin_layout Standard
Zhang 2011
\end_layout

\begin_layout Section
Clustering
\end_layout

\begin_layout Subsection
Shared Nearest Neighbor Similarity (THOMAS)
\end_layout

\begin_layout Subsection
Spectral Clustering (JOSH)
\end_layout

\begin_layout Subsection
Feature Selection???
\end_layout

\begin_layout Standard
Look at clusters and compute entropy vs ALL measures
\end_layout

\begin_layout Part
Discussion
\end_layout

\begin_layout Section
This is the section about math
\end_layout

\begin_layout Standard
The formulation of the Network-Regularized Multiple NMF objective function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
F(W,H_{1},H_{2}) & =||X_{1}-WH_{1}||_{F}^{2}+||X_{2}-WH_{2}||_{F}^{2}\\
 & -\lambda_{1}Tr(H_{2}AH_{2}^{T})-\lambda_{2}Tr(H_{1}BH_{2}^{T})\\
 & +\gamma_{1}||W||_{F}^{2}+\gamma_{2}(\sum_{j}||h_{j}||_{1}^{2}+\sum_{j'}||h_{j'}||_{1}^{2})\\
 & +\lambda_{3}||H_{1}^{T}H_{2}-B||
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The formulation for our problem:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
F(H_{1},H_{2}) & =||X_{1}-H_{1}^{T}H_{2}||_{F}^{2}+||X_{2}-H_{1}^{T}H_{2}||
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
These are the variables we are using.
 
\begin_inset Formula $X_{1}$
\end_inset

 is a subjects x (2x) image features.
 
\begin_inset Formula $X_{2}$
\end_inset

 is subjects x (2x) gene expression.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\hat{X_{1}} & =\begin{bmatrix}1 & 1 & -1 & 1\\
1 & -1 & -1 & -1\\
1 & -1 & 1 & -1
\end{bmatrix}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
X_{1} & =\begin{bmatrix}\begin{bmatrix}1 & 1 & 0 & 1\\
1 & 0 & 0 & 0\\
1 & 0 & 1 & 0
\end{bmatrix} & \begin{bmatrix}0 & 0 & 1 & 0\\
0 & 1 & 1 & 1\\
0 & 1 & 0 & 1
\end{bmatrix}\end{bmatrix}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
F(W,H_{1},H_{2}) & ={\displaystyle \sum_{I=1,2}}||X_{I}-WH_{I}||_{F}^{2}\\
\\
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Multiplicative updates
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\min_{W\geq0}\sum_{I=1,2}||X_{I}-WH_{I}||_{F}^{2}+\gamma_{1}||W||^{2}
\]

\end_inset


\begin_inset Formula 
\[
w_{ij}\leftarrow w_{ij}\frac{(W_{1}H_{1}^{T}+X_{2}H_{2}^{T})_{ij}}{(WH_{1}H_{1}^{T}+WH_{2}H_{2}^{2}+\frac{\gamma_{1}}{2}W_{ij}}
\]

\end_inset


\begin_inset Formula 
\[
\min_{H_{1},H_{2}\geq0}\sum_{I=1,2}||X_{I}-WH_{I}||_{F}^{2}-\lambda_{1}Tr(H_{2}AH_{2}^{T})-\lambda_{2}Tr(H_{1}BH_{2}^{T})+\gamma_{2}(\sum_{j}||h_{j}||_{1}^{2}+\sum_{j'}||h_{j'}||_{1}^{2})
\]

\end_inset


\begin_inset Formula 
\[
h_{ij}^{1}\leftarrow h_{ij}^{1}\frac{(W^{T}X_{1}+\frac{\lambda_{2}}{2}H_{2}B^{T})_{ij}}{[(W^{T}W+\gamma_{2}e_{k\times k})H_{2}]_{ij}}
\]

\end_inset

 
\begin_inset Formula 
\[
h_{ij}^{2}\leftarrow h_{ij}^{2}\frac{(W^{T}X_{2}+\lambda_{1}H_{2}A+\frac{\lambda_{2}}{2}H_{1}B^{T})_{ij}}{[(W^{T}W+\gamma_{2}e_{k\times k})H_{2}]_{ij}}
\]

\end_inset


\end_layout

\begin_layout Standard
tri-matrix factorization
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\min_{F\geq0,S\geq0,G\geq0}\frac{1}{2}(||X-FSG^{T}||_{F}^{2}+\lambda_{F}||F||_{1}^{2}+\lambda_{S}||S||_{1}^{2}+\lambda_{G}||G||_{1}^{2})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
F_{ij}\leftarrow F_{ij}\frac{(XGS^{T})_{ij}}{(FSG^{T}GS^{T})_{ij}+\lambda_{F}||F||_{1}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
S_{ij}\leftarrow S_{ij}\frac{(F^{T}XG)_{ij}}{(F^{T}FSG^{T}G)_{ij}+\lambda_{S}||S||_{1}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
G_{ij}\leftarrow G_{ij}\frac{(X^{T}FS)_{ij}}{(GS^{T}F^{T}FS)_{ij}+\lambda_{G}||G||_{1}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "linear algebra project"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
