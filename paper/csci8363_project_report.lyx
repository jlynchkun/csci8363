#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing other 1.1000000000000001
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Non-negative matrix factorization for the analysis of genotype and phenotype
 data and its relation to patient prognosis and outcome
\end_layout

\begin_layout Author
Thomas Christie and Joshua Lynch, with guidance from TaeHyun Hwang
\end_layout

\begin_layout Part
Introduction 
\end_layout

\begin_layout Section
Motivation
\end_layout

\begin_layout Standard
The availability of detailed bioinformatic data is growing quickly, but
 reliable methods of relating this wealth of information to patient prognosis
 and diagnosis are often lacking.
 In the medical domain, biometric data often consists of an array of features
 for each patient.
 These features may be simple measures such as height and weight, or the
 results of complex analyses such as the presence or absence of certain
 genetic sequences in a patient's DNA.
 
\end_layout

\begin_layout Standard
With the recent explosive growth in computing power, it is becoming more
 common to collect all available data and determine relationships between
 data values and patient prognosis and outcomes after the fact.
 For example, recent studies have shown that collected gene expression data
 (
\begin_inset CommandInset citation
LatexCommand cite
key "deMJ:TheNewEnglandJournalOfMedicine:2002"

\end_inset

) and quantatized features from breast cancer images (
\begin_inset CommandInset citation
LatexCommand cite
key "Beck:SciTranslMed:2011"

\end_inset

) are predictive of cancer metastasis.
 Though this 
\begin_inset Quotes eld
\end_inset

data-driven
\begin_inset Quotes erd
\end_inset

 analysis can lead to the production of unnecessarily large amounts of data,
 there are many bnefits to this approach.
 First, data is not deemed useless apriori due to the verdict of domain
 experts, and this can lead to the discovery of previously unexplored relationsh
ips.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "Beck:SciTranslMed:2011"

\end_inset

, image features were discovered that predicted patient outcome better than
 features traditionally used by physicians.
 Moreover, an active research is being conducted to develop methods of combining
 data from different sources (see e.g.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang:Bioinformatics:2011"

\end_inset

 for an integration of gene expression and micro-RNA data).
\end_layout

\begin_layout Standard
In this investigation, we use a linear algebra approach to uncover structure
 in biometric data and its relationship to patient prognosis and outcome.
 In what follows, we introduce a method based on non-negative matrix factorizati
on for selecting features relevant to patient labels and thereby reducing
 the dimensionality of patient data.
 We then use non-negative matrix bifactorization to uncover structure underlying
 patient data that can successfully separate patients into groups of high-
 and low-survival outcome.
 Lastly, we use spectral clustering to investigate the ability of features
 to successfully separate patients into meaningful groups and demonstrate
 the utility of feature selection.
 
\end_layout

\begin_layout Section
Mathematical Approach
\end_layout

\begin_layout Standard
In the case of biometric data, it can be useful to assume that an underlying
 
\begin_inset Quotes eld
\end_inset

cause
\begin_inset Quotes erd
\end_inset

 is expressed in several of the collected features.
 For example, the presence of a specific gene can give rise to the synthesis
 of many proteins.
 Similarly, a single disease may give rise to a collection of phenotypes.
 One way to model this relationship is to posit a latent causal or explanatory
 structure underlying the measurable data (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:svd"

\end_inset

).
 Discovering hidden explanatory features can potentially lead to a simple
 explanation for various features, as well as drastically reduce the dimensional
ity of the data.
 Though the domain is drastically different, a simliar approach is often
 used to uncover semantic content underlying language data.
 
\end_layout

\begin_layout Subsection
Matrix Decomposition
\end_layout

\begin_layout Standard
To model the relationship between latent variables and measurable features,
 we assume that each feature is a linear combination of some number of latent
 variables.
 Mathematically, the features measured for each patient can be written as
 a vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 , where 
\begin_inset Formula $\mathbf{X}=[x_{1}x_{2}...x_{n}]^{T}$
\end_inset

 is an 
\begin_inset Formula $n\times m$
\end_inset

 matrix with 
\begin_inset Formula $n$
\end_inset

 patients and 
\begin_inset Formula $m$
\end_inset

 features.
 In order to uncover the latent factors, we decompose 
\begin_inset Formula $X$
\end_inset

 into two factors 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $H$
\end_inset

 such that 
\begin_inset Formula $X\approx WH$
\end_inset

.
 In this factorization, 
\begin_inset Formula $W$
\end_inset

 is 
\begin_inset Formula $n\times k$
\end_inset

 matrix whose component vectors span the column space of 
\begin_inset Formula $X$
\end_inset

, where entries of 
\begin_inset Formula $H$
\end_inset

 provide vector weights.
 In other words, 
\begin_inset Formula $X$
\end_inset

 consists of a linear combination of the column vectors of 
\begin_inset Formula $W$
\end_inset

 with weights from 
\begin_inset Formula $H$
\end_inset

.
 
\begin_inset Formula $W$
\end_inset

 provides the 
\begin_inset Formula $k$
\end_inset

 
\begin_inset Quotes eld
\end_inset

sources
\begin_inset Quotes erd
\end_inset

 or latent variables and 
\begin_inset Formula $H$
\end_inset

 describes how they linearly combine to produce the features in 
\begin_inset Formula $X$
\end_inset

.
 
\begin_inset Formula $WH$
\end_inset

 is an approximation of 
\begin_inset Formula $X$
\end_inset

, and the quality of the approximation is determined partially by the choice
 of 
\begin_inset Formula $k$
\end_inset

.
 The dimension 
\begin_inset Formula $k$
\end_inset

 is often chosen heuristically, and different values of 
\begin_inset Formula $k$
\end_inset

 prove useful in different problem domains.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/svd.png
	width 4in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Matrix factorization attempts to find a collection of vectors that can be
 linearly combined to create measured feature vectors.
 The discovered vectors can be considered 
\begin_inset Quotes eld
\end_inset

latent source features
\begin_inset Quotes erd
\end_inset

 that combine to explain feature combinations manifest in the data.
 An optional orthogonality constraint can be placed on source vectors.
 Additionally, a lower number of source vectors can be used to produce an
 approximation of the feature matrix, resulting in an approximation of the
 data matrix with reduced dimensionality.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:svd"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Matrix factorization is not inherently unique, so an important question
 is how to decompose 
\begin_inset Formula $X$
\end_inset

 into 
\begin_inset Formula $H$
\end_inset

 and 
\begin_inset Formula $W$
\end_inset

.
 One principled way to perform such a factorization is to find component
 vectors that are mutually orthogonal.
 This is called Singular Value Decomposition (SVD), and constructs a unique
 factorization 
\begin_inset Formula $X=USV^{T}$
\end_inset

, where 
\begin_inset Formula $U$
\end_inset

 corresponds to 
\begin_inset Formula $W$
\end_inset

 and the latter product corresponds to 
\begin_inset Formula $H$
\end_inset

.
 This approach is mathematically robust and well-understood.
 A lower-rank approximation of 
\begin_inset Formula $X$
\end_inset

 can be constructed using the first 
\begin_inset Formula $k$
\end_inset

 columns of 
\begin_inset Formula $U$
\end_inset

, 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $V^{T}$
\end_inset

.
 
\end_layout

\begin_layout Standard
From the perspective of uncovering latent sources, however, SVD presents
 several problems.
 First, it may well be the case that latent sources are not actually orthogonal.
 In this case, the 
\begin_inset Formula $U$
\end_inset

 matrix would not correspond to any domain-specific functional process but
 would be a mere mathematical convenience.
 In addition, SVD does not impose a constraint on the sign of values of
 
\begin_inset Formula $H$
\end_inset

.
 A combination of positive and negative coefficients create both additive
 and subtractive components of each feature, making it very difficult to
 interpret how latent sources combine to create features.
 Lastly, SVD produces results which are dense (few 0s in the components
 of 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $H$
\end_inset

).
 This implies that every latent source has an effect on every feature present
 in 
\begin_inset Formula $X$
\end_inset

, an implication which is likely not true in many applications.
 Approaches for addressing mixed signs, orthogonality and density are addressed
 below.
\end_layout

\begin_layout Subsection
Non-Negative Matrix Factorization (NMF)
\end_layout

\begin_layout Standard
One approach to creating easily interpretable latent sources is to introduce
 the additional requirement that the data have values greater than or equal
 to zero and perform matrix decomposition in such a way that values of 
\begin_inset Formula $H$
\end_inset

 and 
\begin_inset Formula $W$
\end_inset

 are exclusively nonnegative.
 This means that the underlying components of 
\begin_inset Formula $X$
\end_inset

 combine in a purely additive way to produce 
\begin_inset Formula $X$
\end_inset

, which gives a more intuitive decomposition of the data matrix.
 Moreover, in situations where data is naturally nonnegative (such as image
 data), negative components may not be readily interpretable and non-negative
 decomposition is preferred.
 
\end_layout

\begin_layout Standard
Biometric data often originates with mixed signs, either inherently or due
 to common data transforms.
 When faced with a 
\begin_inset Formula $n\times m$
\end_inset

 matrix 
\begin_inset Formula $X$
\end_inset

 with mixed-sign entries, a common technique for imposing nonnegativity
 is to construct a new 
\begin_inset Formula $n\times2m$
\end_inset

 matrix 
\begin_inset Formula $X'=[\max(X,0)\max(-X,0)]$
\end_inset

.
 In other words, the left half of 
\begin_inset Formula $X'$
\end_inset

 consists of the originally positive values in 
\begin_inset Formula $X$
\end_inset

 with the negative values replaces by 0.
 The right half of 
\begin_inset Formula $X$
\end_inset

 consists of the absolute value of originally negative values of
\begin_inset Formula $X$
\end_inset

, with 0's replacing corresponding positive values.
 For example:
\begin_inset Formula 
\begin{align*}
\left[\begin{array}{ccc}
1 & -2 & -1\\
2 & 5 & 3\\
2 & -4 & 4
\end{array}\right] & \rightarrow\left[\begin{array}{cccccc}
1 & 0 & 0 & 0 & 2 & 1\\
2 & 5 & 3 & 0 & 0 & 0\\
2 & 0 & 4 & 0 & 4 & 0
\end{array}\right]
\end{align*}

\end_inset

This is the approached used in the methods below.
\end_layout

\begin_layout Standard
Placing an orthogonality constraint on the vectors of the basis matrix 
\begin_inset Formula $W$
\end_inset

 forces components to be uncorrelated.
 While this is useful from a data compression standpoint and has other convenien
t mathematical properties, it may well be the case that distinct underlying
 sources have overlapping manifestations in feature space.
 In the approaches used in this paper, the orthogonality constraint is dropped.
\end_layout

\begin_layout Subsection
Sparse Decomposition
\end_layout

\begin_layout Standard
A matrix is called sparse when the majority of its elements are 0.
 In cases where a sparse 
\begin_inset Formula $H$
\end_inset

 can be found, the product 
\begin_inset Formula $WH$
\end_inset

 can be seen as an approximation of 
\begin_inset Formula $X$
\end_inset

 in which each feature of 
\begin_inset Formula $X$
\end_inset

 is composed of a small number of columns of 
\begin_inset Formula $W$
\end_inset

.
 This is useful, as it highlights which latent components are most responsible
 for each feature.
 Note that with sparse data, even through the orthogonality constraint is
 relaxed, columns have a high probability of being orthogonal (for a more
 detailed explanation see 
\begin_inset CommandInset citation
LatexCommand cite
key "Kim:Bioinformatics:2007"

\end_inset

).
 
\end_layout

\begin_layout Subsection
Matrix Decomposition as Optimization
\end_layout

\begin_layout Standard
Factoring a 
\begin_inset Formula $nxm$
\end_inset

 matrix 
\begin_inset Formula $X$
\end_inset

 into a product 
\begin_inset Formula $WH$
\end_inset

 can be formulated as an optimization problem:
\begin_inset Formula 
\begin{equation}
\min_{W,H}f(W,H)=\frac{1}{2}||X-WH||^{2}\label{eq:bifactorization-optimization}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $||\cdot||$
\end_inset

 is a convenient matrix norm, 
\begin_inset Formula $W\epsilon\mathbb{R}^{n\times k}$
\end_inset

 and 
\begin_inset Formula $H\epsilon\mathbb{R}^{k\times m}$
\end_inset

.
 If 
\begin_inset Formula $X$
\end_inset

 is non-negative, non-negativity can be imposed as an additional constraint
 on 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $H$
\end_inset

.
 Sparsity can be controlled by adding a term penalizing the size of the
 
\begin_inset Formula $L^{0}$
\end_inset

 norm of 
\begin_inset Formula $W$
\end_inset

 and/or 
\begin_inset Formula $H$
\end_inset

.
 In practice the 
\begin_inset Formula $L^{1}$
\end_inset

 norm is often used as an approximation of the 
\begin_inset Formula $L^{0}$
\end_inset

 norm, as using the 
\begin_inset Formula $L^{1}$
\end_inset

 norm gives a convex optimiation problem while still enforcing sparsity.
 There are many well-studied algorithms for efficiently solving convex optimizat
ion problems.
\end_layout

\begin_layout Subsubsection
Convex Optimization
\end_layout

\begin_layout Standard
The general problem of factoring a matrix into a product of two or more
 matrices is under-specified even with additional constraints such as non-negati
vity.
 In fact, requiring a non-negative factorization results in a more difficult
 problem than a simple unconstrained decomposition such as SVD because in
 such cases no closed-form solution exists and results are not guaranteed
 to be unique.
 For the most part these problems are formulated as optimization problems
 and solved iteratively.
 Solving an optimization problem such as 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bifactorization-optimization"

\end_inset

 efficiently with guarantees of convergence and with known error bounds
 is in general difficult.
 If the problem can be converted to a convex optimization it may become
 tractable.
 Several techniques may be used including 'relaxation' to a convex norm
 (substituting a convex norm for a non-convex norm), and alternately optimizing
 one matrix while holding the others constant.
 The factorization methods used in this paper utilize both of these techniques.
\end_layout

\begin_layout Subsubsection
Lagrange Multipliers
\end_layout

\begin_layout Standard
A common approach to incorporating equality and inequality constraints into
 an optimization problem such as 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bifactorization-optimization"

\end_inset

 is to reformulate the problem with additional variables, one for each constrain
t.
 This new formulation is the Lagrangian dual problem and the new variables
 are analogous to Lagrange multipliers.
 For convex optimization problems the solution to the Lagrange dual problem
 may be the exact solution to the original, or primal, problem.
 Even when the primal and dual solutions are not identical the dual solution
 is at least a lower bound on the solution to the primal problem.
\end_layout

\begin_layout Subsection
Data Sources and Preparation
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status collapsed

\begin_layout Plain Layout
gene expression subject count: 248 
\end_layout

\begin_layout Plain Layout
gene expression feature count: 3550 
\end_layout

\begin_layout Plain Layout
image data subject count: 248 
\end_layout

\begin_layout Plain Layout
image data feature count: 6642 
\end_layout

\begin_layout Plain Layout
86 subjects have label "metastasis" 
\end_layout

\begin_layout Plain Layout
162 subjects have label "non-metastasis" 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Two datasets were used in the investigations below.
 The first, which we term the Gene-Image dataset, consists of both gene
 expression information and cell image feature information from cancerous
 cells.
 The gene and image components were acquired from 
\begin_inset CommandInset citation
LatexCommand cite
key "deMJ:TheNewEnglandJournalOfMedicine:2002"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "Beck:SciTranslMed:2011"

\end_inset

, respectively.
 
\end_layout

\begin_layout Standard
The complete gene expression dataset contains 24,481 features for 248 subjects.
 This data was preprocessed to remove expression information related to
 genes for which no name was provided.
 Also, gene expression features with absolute values lower than the 60th
 percentile were removed, as were gene expression values with variance lower
 than the 30th percentile.
 After preprocessing 3,550 expression features remained, which were then
 standardized to have zero mean and unit variance.
\end_layout

\begin_layout Standard
The complete image feature data set contains 6,642 features for the same
 248 subjects.
 All 6,642 features were used.
 As with the gene expression data, the cell image features were standardized
 to zero mean and unit variance.
\end_layout

\begin_layout Standard
The gene expression and cell image data both contained negative feature
 values.
 In order to use multiplicative updates, every entry in the matrix to be
 factored must be positive.
 After column-wise standardization, each data matrix was doubled along the
 columns to impose non-negativity as described above.
 
\end_layout

\begin_layout Standard
Each patient was labeled as having cancer which had metastasized or not,
 and duration of patient survival following treatment onset was also included.
\end_layout

\begin_layout Standard
The second dataset is called the Wisconsin Diagnostic Breast Cancer dataset
 (CITE) (the simpler 'Wisconsin Dataset' is used below), collected in the
 lab of Dr.
 William H.
 Wolberg at the University of Wisconsin.
 The data consists of 10 image features extracted from each of three cells
 from 569 subjects.
 Only two cells (20 total features) were used, and the feature matrix was
 normalized and made non-negative.
 Each patient was labeled as having benign or malignant tumors.
\end_layout

\begin_layout Part
Experiments
\end_layout

\begin_layout Standard
The goal of this project was to evaluate the use of non-negative matrix
 factorization as a tool for uncovering structure in large datasets.
 With this in mind, we had three goals:
\end_layout

\begin_layout Enumerate
Given a dataset with patients, corresponding features, and labels, to use
 non-negative matrix factorization to uncover features, latent and actual,
 most relevant to patient labels.
\end_layout

\begin_layout Enumerate
Given the same dataset and two different types of features, to determine
 whether a common underlying structure (or common basis, in mathematical
 terms) in those features relates to patient survival.
 
\end_layout

\begin_layout Enumerate
To perform spectral clustering on the dataset both in a naive fashion (using
 all features) and using the matrix factorization-based feature selection
 developed below.
\end_layout

\begin_layout Section
Supervised Approach
\end_layout

\begin_layout Standard
In this section, we used non-negative matrix factorization to uncover features
 and feature combinations related to patient tumor category.
 
\end_layout

\begin_layout Subsection
Non-negative Matrix Tri-factorization
\end_layout

\begin_layout Standard
In traditional SVD, a matrix 
\begin_inset Formula $X$
\end_inset

 is factored into three matrices 
\begin_inset Formula $X=USV^{T}$
\end_inset

 in such a way that the columns of 
\begin_inset Formula $U$
\end_inset

 and 
\begin_inset Formula $V$
\end_inset

 are orthonormal and are typically composed of mixed-sign entries.
 Columns of 
\begin_inset Formula $U$
\end_inset

 are a basis for the column space of 
\begin_inset Formula $X$
\end_inset

, and are combinations of values that can be thought of as 'latent features'
 composed of combinations of the original columns such that they point in
 the direction of greatest variance, next greatest, and so on for each column
 of 
\begin_inset Formula $U$
\end_inset

.
 
\end_layout

\begin_layout Standard
In an analogous way, we perform a non-negative matrix tri-factorization
 that uncovers latent features that account for the greatest variation in
 our data.
 If 
\begin_inset Formula $X$
\end_inset

 is composed of two concatenated feature sets, the latent features are linear
 combinations of the original features, and thereby provide insight into
 which features combine to predict variability in the patient labels.
\end_layout

\begin_layout Standard
In order to maintain consistency with other non-negative matrix factorization
 literature, we use the matrix labels 
\begin_inset Formula $F$
\end_inset

, 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

 to represent the three factors of 
\begin_inset Formula $X$
\end_inset

.
 Each row of 
\begin_inset Formula $X$
\end_inset

 corresponds to a subject, and each column corresponds to a feature.
 Since we are attempting to uncover features and feature combinations as
 they are relevant to patient labels (and not to overall variability among
 patients), we will fix the matrix 
\begin_inset Formula $F$
\end_inset

 and use it to encode patient labels.
 Unlike SVD, non-negative matrix factorization is non-unique, and because
 we are drastically constraining the values of F we are actually computing
 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G^{T}$
\end_inset

 in such a way that 
\begin_inset Formula $FSG^{T}$
\end_inset

 most closely approximates 
\begin_inset Formula $X$
\end_inset

 rather than factoring it exactly.
 From this perspective, the factorization is turned into a minimization
 problem, namely computing non-negative 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

 in a way that minimizes 
\begin_inset Formula $||X-FSG^{T}||_{F}$
\end_inset

, where 
\begin_inset Formula $||\cdot||_{F}$
\end_inset

 indicates the Frobenius norm, or rooted sum of squared elements, of a matrix.
 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X_{1}$
\end_inset

 and 
\begin_inset Formula $X_{2}$
\end_inset

 be the two non-negative subject-by-feature matrices with common subjects
 and different feature sets, and let 
\begin_inset Formula $L$
\end_inset

 be a vector of binary labels for subjects.
 We then create an matrix 
\begin_inset Formula $X=[X_{1}X_{2}]$
\end_inset

 as a concatenated subject-by-feature matrix.
 Let 
\begin_inset Formula $F$
\end_inset

 be a 2-column matrix such that entries in the first column of 
\begin_inset Formula $F$
\end_inset

 in rows where 
\begin_inset Formula $L$
\end_inset

 is 1, and entries in the second column of 
\begin_inset Formula $F$
\end_inset

 are 1 where 
\begin_inset Formula $L$
\end_inset

 is 0.
 
\begin_inset Formula $S$
\end_inset

 is a matrix with 2 rows and 
\begin_inset Formula $k$
\end_inset

 columns, where 
\begin_inset Formula $k$
\end_inset

 is the number of latent feature vectors to be discovered.
 Finally, 
\begin_inset Formula $m$
\end_inset

 is the total number of concatenated non-negative feature vectors, and 
\begin_inset Formula $G$
\end_inset

 is a 
\begin_inset Formula $m\times k$
\end_inset

 matrix such that eacy row corresponds to a non-negative feature and each
 column corresponds to a latent feature to be discovered.
 As described below, 
\begin_inset Formula $k$
\end_inset

 is chosen so that the rank of 
\begin_inset Formula $G$
\end_inset

 corresponds to the number of singular values that explain roughly 90% of
 the variance in 
\begin_inset Formula $X$
\end_inset

.
 A visual representation of the matrices can be seen in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:tri-matirx-visualization"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/tri-factorization.png
	width 5in
	rotateOrigin centerTop

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Non-negative tri-factorization schematic using patient labels.
 
\begin_inset Formula $X$
\end_inset

 corresponds to the data matrix, 
\begin_inset Formula $F$
\end_inset

 to enforced labels, and 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

 are learned matrices.
 
\begin_inset Formula $M1$
\end_inset

 and 
\begin_inset Formula $M2$
\end_inset

 correspond to the data from different sources that are concatenated to
 create 
\begin_inset Formula $X$
\end_inset

.
 
\begin_inset Formula $N$
\end_inset

 is the number of subjects, and 
\begin_inset Formula $k$
\end_inset

 is the number of latent features in 
\begin_inset Formula $G$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:tri-matirx-visualization"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Problem Formulation (incl multiplicative updates)
\end_layout

\begin_layout Standard
Keeping in mind that we desire sparse and non-negative 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

, the problem described above can be written as the following:
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\begin{equation}
\min_{S\geq0,G\geq0}\frac{1}{2}(||X-FSG^{T}||_{F}^{2}+\lambda_{S}\sum_{j}||s_{j}||_{1}^{2}+\lambda_{S}\sum_{j'}||g_{j'}||_{1}^{2})\label{eq:tri-minimization}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where both 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $F$
\end_inset

 are fixed.
 The two additional 1-norm terms serve to encourage sparsity in 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

, and represent a sum of the absolute values of each entry of each column,
 all summed and then the result squared.
 Since entries in 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

 are non-negative, the terms represent the sum of all elements of 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

, respectively.
 Sparsity is ideally computed using the so-called 0-norm, which is the count
 of non-zero elements of a matrix.
 The 1-norm is a commonly used relaxation of the 0-norm, which is the closest
 norm to the 0-norm that is also convex, and can therefore be optimized
 using convex optimization methods.
\end_layout

\begin_layout Standard
Following a similar formulation in 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang:Bioinformatics:2011"

\end_inset

, the objective function corresponding to 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:tri-minimization"

\end_inset

 can be written 
\begin_inset Formula 
\begin{equation}
\mathcal{F}(S,G)=\frac{1}{2}(||X-FSG^{T}||_{F}^{2}+\lambda_{S}\sum e_{1\times2}SS^{T}e_{2\times1}^{T}+\lambda_{G}\sum e_{1\times k}GG^{T}e_{k\times1}^{T})\label{eq:trimatrix-objective-function}
\end{equation}

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 with the constraints 
\begin_inset Formula $G\geq0$
\end_inset

 and 
\begin_inset Formula $S\geq0$
\end_inset

.
 The latter two terms again represent the sum of entries 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

 squared, and 
\begin_inset Formula $e$
\end_inset

 is a vector of 1s.
\end_layout

\begin_layout Standard
Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:trimatrix-objective-function"

\end_inset

 contains two variables and is therefore not simultaneously convex in both,
 but only in each individually.
 To achieve convexity and minimize the objective function, we fix either
 
\begin_inset Formula $S$
\end_inset

 or 
\begin_inset Formula $G$
\end_inset

, update the other, and iterate until a convergence criteria is met.
 In order to construct a method of calculating 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

 independently, we first construct the Lagrange 
\begin_inset Formula $\mathcal{L}$
\end_inset

.
 Let 
\begin_inset Formula $\phi_{i}{}_{j}$
\end_inset

 and 
\begin_inset Formula $\psi_{i}{}_{j}$
\end_inset

 be the Lagrange multipliers for the constraints 
\begin_inset Formula $S\geq0$
\end_inset

 and 
\begin_inset Formula $G\geq0$
\end_inset

, respectively.
 Then 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{L}(S,G)=\mathcal{F}+Tr(\Phi S^{T})+Tr(\Psi G^{T})
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\Phi=[\phi_{i}{}_{j}]$
\end_inset

 and 
\begin_inset Formula $\Psi=[\psi_{i}{}_{j}]$
\end_inset

.
 The partial derivatives of 
\begin_inset Formula $\mathcal{L}$
\end_inset

 with respect to 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

 are:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial\mathcal{L}}{\partial S}=-F^{T}XG+F^{T}FSG^{T}G+\lambda_{S}e_{2\times2}S+\Phi
\]

\end_inset

 
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\[
\frac{\partial\mathcal{L}}{\partial G}=-X^{T}FS+GS^{T}F^{T}FS+\lambda_{G}e_{k\times k}G+\Psi
\]

\end_inset


\end_layout

\begin_layout Standard
Based on the KKT conditions 
\begin_inset Formula $\phi_{i}{}_{j}S_{i}{}_{j}=0$
\end_inset

 and 
\begin_inset Formula $\psi_{i}{}_{j}G_{i}{}_{j}=0$
\end_inset

, we get the following equations for 
\begin_inset Formula $S_{i}{}_{j}$
\end_inset

 and 
\begin_inset Formula $G_{i}{}_{j}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
(-F^{T}XG)_{i}{}_{j}S_{i}{}_{j}+(F^{T}SG^{T}G+\lambda_{S}e_{2\times2}S)_{i}{}_{j}S_{i}{}_{j}=0
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
(-X^{T}FS)_{i}{}_{j}G_{i}{}_{j}+(GS^{T}F^{T}FS+\lambda_{G}e_{k\times k}G)_{i}{}_{j}G_{i}{}_{j}=0
\]

\end_inset


\end_layout

\begin_layout Standard
Moving the first term of each to the right side of the equal sign and dividing
 by the first part of the second term gives the following update rules,
 which are used iteratively:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
S_{i}{}_{j}\leftarrow S_{i}{}_{j}\frac{(F^{T}XG)_{i}{}_{j}}{(F^{T}SG^{T}G+\lambda_{S}e_{2\times2}S)_{i}{}_{j}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
G_{i}{}_{j}\leftarrow G_{i}{}_{j}\frac{X^{T}FS}{(GS^{T}F^{T}FS+\lambda_{G}e_{k\times k}G)_{i}{}_{j}}
\]

\end_inset


\end_layout

\begin_layout Standard
If the values are initialized as non-negative, the update rules are guaranteed
 to be non-increasing and are only stationary if 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

 are at a stationary point (see 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang:Bioinformatics:2011"

\end_inset

 supplementary material for a proof).
 In practice, a small epsilon is added to the denominator of each update
 rule to preclude devision by zero.
 Additionally, 
\begin_inset Formula $F$
\end_inset

 is column-normalized and 
\begin_inset Formula $G$
\end_inset

 is row-normalized at each iteration so that 
\begin_inset Formula $S$
\end_inset

 absorbs the scaling from both matrices.
\end_layout

\begin_layout Standard
As the update rules are multiplicative and each new value of 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

 is a function of past values, 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

 must first be initialized.
 The matrix
\begin_inset Formula $F$
\end_inset

 contains patient labels and can be thought of as an indicator matrix for
 clustering patients.
 Correspondingly, the matrix 
\begin_inset Formula $G$
\end_inset

 can be thought of as clustering features, which correspond to columns of
 
\begin_inset Formula $X$
\end_inset

.
 With this in mind, we initialize 
\begin_inset Formula $G$
\end_inset

 using k-means clustering on columns of 
\begin_inset Formula $X$
\end_inset

, where the number of clusters used is the number of columns (latent features)
 in 
\begin_inset Formula $G$
\end_inset

.
 The resulting matrix is an 
\begin_inset Formula $n\times k$
\end_inset

 indicator matrix of feature clusters in which each column has a single
 1 representing cluster membership and all other entries are 0.
 The initial 
\begin_inset Formula $G$
\end_inset

 is set to this matrix.
 
\begin_inset Formula $S$
\end_inset

 is initialized to random numbers between 0 and 1.
\end_layout

\begin_layout Subsection
Feature Selection
\end_layout

\begin_layout Standard
As mentioned above, rows of 
\begin_inset Formula $F$
\end_inset

 indicate an enforced clustering on subjects corresponding to clinical patient
 labels, in which each row contains a zero and nonzero entry (not 1 because
 of column-wise scaling).
 Group membership can be determined by assigning a subject to the column-label
 with corresponding highest value in the subject's row in 
\begin_inset Formula $F$
\end_inset

.
 Since 
\begin_inset Formula $F$
\end_inset

 is predetermined and fixed, this merely represents a change in interpretation
 from label to cluster membership.
 
\end_layout

\begin_layout Standard
Once 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

 are learned, we can consider the 
\begin_inset Formula $2\times m$
\end_inset

 product 
\begin_inset Formula $SG^{T}$
\end_inset

, where 
\begin_inset Formula $m$
\end_inset

 is the total number of non-negative features.
 This can be thought of as an indicator matrix similar to 
\begin_inset Formula $F$
\end_inset

, except that each feature is assigned to one of 2 feature clusters.
 Unlike the random initialization of 
\begin_inset Formula $G$
\end_inset

, in which features were clustered without taking into account patient labels,
 the learned 
\begin_inset Formula $SG^{T}$
\end_inset

 represents feature cluster membership as they relate to the patient labels.
 In other words, each (normalized) column of
\begin_inset Formula $SG^{T}$
\end_inset

 represents a posterior probability of that feature being relevant to classifyin
g a patient in one label or the other.
 
\end_layout

\begin_layout Standard
When considered as probabilities, it is reasonable to state that the higher
 a probability corresponding to one label is, the stronger the corresponding
 feature will be in predicting class membership.
 On the other hand, for features in which both probabilities are similar
 (both roughly 0.5), the predictive value of that feature is relatively small.
\end_layout

\begin_layout Standard
We tested this method using the Wisconsin dataset.
 As indicated above, 
\begin_inset Formula $F$
\end_inset

 was created by normalizing a binary indicator matrix for feature labels.
 
\begin_inset Formula $S$
\end_inset

 was initialized randomly, and 
\begin_inset Formula $G$
\end_inset

 was initialized by choosing the k-means clustering result with the lowest
 SSE over 10 runs.
 
\begin_inset Formula $k$
\end_inset

 was chosen to be 29, which corresponds to the number of singular values
 of the data matrix corresponding to 90% of the variance of the data matrix.
\end_layout

\begin_layout Standard
The results of the tri-factorization are shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Tri-factorization-of-Wisconsin"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/tri-factorization-wisconsin-results.png
	width 6in

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Tri-factorization of modified Wisconsin Diagnostic Breast Cancer dataset
 
\begin_inset Formula $X$
\end_inset

 into 
\begin_inset Formula $F$
\end_inset

,
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

, respectively.
\begin_inset CommandInset label
LatexCommand label
name "fig:Tri-factorization-of-Wisconsin"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Class labels in 
\begin_inset Formula $F$
\end_inset

 are readily apparent in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Tri-factorization-of-Wisconsin"

\end_inset

.
 As desired, 
\begin_inset Formula $G$
\end_inset

 is sparse and 
\begin_inset Formula $S$
\end_inset

 approximately so, with only a few values being relatively very high.
 It is not immediately apparent how the features of 
\begin_inset Formula $W$
\end_inset

 correspond to the labels in 
\begin_inset Formula $F$
\end_inset

, however.
 Multiplying 
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $G^{T}$
\end_inset

 gives a much clearer picture, shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:wisconsin-SG"

\end_inset

.
 It is clear that values corresponding to actual cells typically show much
 stronger contrast between one class and another than the random data.
 It appears that values corresponding to positive feature values (or, the
 high-end of values once features are standardized) are are predictive of
 the first class, whereas lower/negative values tend to be predictive of
 the second class.
 Of course, since there are only two classes, any extreme value is by default
 predictive of membership in both classes.
 The expected ratio between class membership values for non-predictive features
 is equivalent to the proportion of subjects in each group.
 Since there are roughly 1.8 times as many subjects in the second column
 of 
\begin_inset Formula $F$
\end_inset

 in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Tri-factorization-of-Wisconsin"

\end_inset

 as the first, we expect the ratio for non-predictive noise attributes in
 the second row of 
\begin_inset Formula $SG^{T}$
\end_inset

 to be about 1.8, which is indeed what we see.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/tri-factorization-wisconsin-results-SG.png
	width 3.5in

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Formula $SG^{T}$
\end_inset

from the factorization of the modified Wisconsin Diagnostic Breast Cancer
 dataset.
 Columns 1-20 correspond to 10 positive and 10 negative features from Cell
 1.
 Features 21-30 and 41-50 are positive and negative features from Cell 2.
 Features 31-40 and 51-60 are positive and negative random features drawn
 from a normal distribution with mean 1 and standard deviation 0.
 Note that columns are not normalized in this figure.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:wisconsin-SG"

\end_inset


\end_layout

\end_inset

Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SG-ratios"

\end_inset

 displays ratios of the first to the second row as well as the converse.
 Large values in both cases correspond to the highest power of features
 to differentiate between patient classes.
 Several aspects of this figure are worth noting.
 First, entries corresponding to noise features show consistently low ratios
 (close to 1.8).
 Ratios between corresponding positive and negative features from Cell 2
 are both high (in entries 21-30 and 41-50), which suggests that higher-than-ave
rage values predict one class and lower-than-average of the same feature
 predict the other class.
 Finally, features from Cell 2 (entries 21-30 and 41-50) appear to be generally
 stronger predictors than features from Cell 1 (entries 1-20).
\end_layout

\begin_layout Standard
In order to validate the applicability of features with high ratios in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SG-ratios"

\end_inset

, we compare the top values from the top and bottom ratios to the features
 with highest information gain with respect to class labels.
 The top features from Row 1/Row 2 are features 23, 24, 21, 28 and 4, in
 that order.
 These correspond to features 14, 11, 13, 4 and 18 in the mixed-sign 
\begin_inset Formula $X'=[X1X2]$
\end_inset

 matrix of cell features.
 The top features in Row 2 / Row 1 are numbers 43, 41, 48, 47 and 44, which
 correspond to features 13, 11, 18, 17 and 14 in 
\begin_inset Formula $X'$
\end_inset

.
 We used WEKA (CITE) to calculate the information gain with respect to patient
 class.
 According to this measure, the most informative six features are 13, 14,
 11, 18, 4 and 17, all of which are in the top feature ratios from Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SG-ratios"

\end_inset

.
 Thus, non-negative matrix factorization with a class-label matrix 
\begin_inset Formula $F$
\end_inset

 produces a 
\begin_inset Formula $SG^{T}$
\end_inset

 matrix that can readily detect which features are relevant to class labels.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/SG-ratios.png
	width 4in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Ratios between rows 1 and 2 of 
\begin_inset Formula $SG^{T}$
\end_inset

 resulting from tri-matrix factorization.
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:SG-ratios"

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Feature Interaction
\end_layout

\begin_layout Standard
The matrix 
\begin_inset Formula $G^{T}$
\end_inset

 in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Tri-factorization-of-Wisconsin"

\end_inset

 does not readily reveal insights into the relationships between features
 and classes.
 Each entry corresponds to a feature's belonging to a latent variable which
 itself points in a direction of high variation.
 Pairwise correlations between rows of 
\begin_inset Formula $G^{T}$
\end_inset

 reveal which pairs of features have similar presence across latent features,
 and therefore represent an association between features as they relate
 to patient labels.
 Taking correlations between features across subjects in the data matrix
 
\begin_inset Formula $X$
\end_inset

 reveals unrestricted relationships between features, while taking correlations
 between features in the 
\begin_inset Formula $G^{T}$
\end_inset

 matrix acts to filter the correlations, revealing those feature correlations
 that most strongly reflect patient labels.
 These two calculations are compared in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:X-and-G-correlations"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/wisconsin-raw-correlations-x1-x2-R.png
	height 2in

\end_inset


\begin_inset Graphics
	filename figures/wisconsin_tri_fact_Gt_correlation_matrix.png
	height 2in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
The left graphic shows correlations between features in the data matrix
 
\begin_inset Formula $X$
\end_inset

.
 The right graphic shows correlations in the doubled matrix 
\begin_inset Formula $G^{T}$
\end_inset

.
 Rows and columns of both correspond to image features.
 Recall that features 1-20 are image features from two cells, and features
 21-30 are noise features.
 The right correlation matrix is much sparser than the left.
 This is a direct effect of the sparse matrix decomposition, and serves
 to highlight only those correlations that relate to the subject labels.
 However, some correlations in noise features can also be seen in the right
 matrix which are not present in the left.
 
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:X-and-G-correlations"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Unsupervised Approach
\end_layout

\begin_layout Standard
Accurate patient prognosis is a vital part of medical practice.
 When a patient is diagnosed with a disease, the expected course of the
 disease largely determines the choice of treatment.
 Moreover, studies are now finding that patient genetic information is can
 be highly predictive of a patient's response to particular medications.
 Treatments for serious illnesses are often costly and can have permanent
 and debilitating side-effects.
 The ability to accurately predict the utility of various treatments on
 an individual patient is therefore critical for both patient well-being
 and the appropriate allocation of medical funds.
\end_layout

\begin_layout Standard
In what follows, we use non-negative matrix factorization to uncover structure
 in patient biometric data that is predictive of patient survival.
\end_layout

\begin_layout Subsection
Survival analysis
\end_layout

\begin_layout Standard
Kaplan-Meier survival analysis is a commonly used technique for determining
 the ability of a measured feature to significantly predict the lifespan
 of a group of patients.
 To perform the analysis, matched patients are given a treatment and the
 number of years the patient survives during treatment is recorded.
 The goal is to find biometric data that is predictive of survival duration.
 This discovered metric can then be used to determine whether future patients
 should be given the treatment in question.
 In a typical Kaplan-Meier plot, subjects are split into two groups according
 to the value of a biometric marker.
 The fraction of each group surviving at every time point is then plotted.
 The log-rank statistical test is used to determine whether the difference
 between survival curves is statistically significant.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/kaplan-meier-sample.jpg
	width 2in

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Example Kaplan-Meier survival curve.
 (CITE FROM WIKIPEDIA)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The Gene-Image dataset described in the introduction includes survival times
 for each patient.
 Our goal is to use non-negative matrix factorizaton to uncover linear combinati
ons of features that separate patients into groups with significantly different
 survival rates.
 The Gene-Image dataset includes both gene expression and cancer cell image
 feature data.
 Gene expression data is presumably linked in an unknown way to phenotype
 data as expressed in image features.
 Our underlying assumption is that there exists a common set of basis vectors
 underlying these two datasets that contains information regarding patient
 response to cancer and therefore their survival outcome.
 
\end_layout

\begin_layout Standard
Finally, optimally combining data from multiple sources is an active area
 of research in the data mining and bioinformatics communities.
 As part of our approach, we wish to determine whether including a term
 representing potential gene-image interactions has an effect on the ability
 of this method to predict patient outcome.
 
\end_layout

\begin_layout Subsection
Bifactorization with additional constraints
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X_{G}$
\end_inset

 be the non-negative 
\begin_inset Formula $n\times g$
\end_inset

 matrix of gene expression data, prepared as explained in the introduction,
 and let 
\begin_inset Formula $X_{I}$
\end_inset

 be the 
\begin_inset Formula $n\times i$
\end_inset

 matrix of image features.
 Our goal is to discover an approximate common basis for 
\begin_inset Formula $X_{G}$
\end_inset

 and 
\begin_inset Formula $X_{I}$
\end_inset

, and use the basis vectors to separate patients into groups.
 Once separated, we perform Kaplan-Meier survival analysis to determine
 whether the groups have significantly different survival outcome, and therefore
 whether the common basis vectors relate to patient outcome.
 Patient labels are not used in the factorization, so this represents an
 unsupervised approach to uncovering structure in patient biometric data.
\end_layout

\begin_layout Standard
Given 
\begin_inset Formula $X_{G}$
\end_inset

 and 
\begin_inset Formula $X_{I}$
\end_inset

, we wish to find a common matrix of basis vectors 
\begin_inset Formula $W$
\end_inset

 and two coefficient matrices 
\begin_inset Formula $H_{G}$
\end_inset

 and 
\begin_inset Formula $H_{I}$
\end_inset

 such that 
\begin_inset Formula $||X_{G}-WH_{G}||_{F}^{2}+||X_{I}-WH_{I}||_{F}^{2}$
\end_inset

 is minimized.
 It is convenient to impose further constraints on 
\begin_inset Formula $W$
\end_inset

, 
\begin_inset Formula $H_{G}$
\end_inset

 and 
\begin_inset Formula $H_{I}$
\end_inset

.
 Namely, we wish to constrain the size of 
\begin_inset Formula $W$
\end_inset

 and encourage sparse 
\begin_inset Formula $H_{G}$
\end_inset

 and 
\begin_inset Formula $H_{I}$
\end_inset

.
 Sparse coefficient matrices ensure that each data matrix is composed of
 small linear combination of basis vectors, such that the most important
 component vectors are emphasized and less significant components are forced
 to 0.
 The nature of our data also provides an additional constraint: known gene-gene
 interactions are included in a matrix 
\begin_inset Formula $A$
\end_inset

 (provided by TaeHyun Hwang) as prior knowledge that serve to force covariance
 between gene expression values that are known to be interrelated.
 Finally, hypothetical and unknown gene expression / image feature relationships
 can be represented by a matrix 
\begin_inset Formula $B$
\end_inset

.
 The complete objective function to be minimzed is given in Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bi-factorization-objective-function"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\mathcal{F}(W,H_{G},H_{I}) & =||X_{G}-WH_{G}||_{F}^{2}+||X_{I}-WH_{I}||_{F}^{2}\nonumber \\
 & -\lambda_{1}Tr(H_{G}AH_{G}^{T})-\lambda_{2}Tr(H_{I}BH_{G}^{T})\nonumber \\
 & +\gamma_{1}||W||_{F}^{2}+\gamma_{2}(\sum_{j}||h_{j}||_{1}^{2}+\sum_{j'}||h_{j'}||_{1}^{2})\label{eq:bi-factorization-objective-function}\\
 & +\lambda_{3}||H_{I}^{T}H_{G}-B_{0}||_{F}^{2}\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Standard
The first line indicates the factor approximation of 
\begin_inset Formula $X_{G}$
\end_inset

 and 
\begin_inset Formula $X_{I}$
\end_inset

 to be computed.
 Terms in the second line represent gene-gene interaction and gene-image
 relationships, respectively.
 They are subtracted as we wish to compute 
\begin_inset Formula $W$
\end_inset

, 
\begin_inset Formula $H_{G}$
\end_inset

 and 
\begin_inset Formula $H_{I}$
\end_inset

 in such a way that these interactions are taken into account.
 The third line contains a term penalizing the growth of 
\begin_inset Formula $W$
\end_inset

, as well as a term encouraging sparse 
\begin_inset Formula $H_{G}$
\end_inset

 and 
\begin_inset Formula $H_{I}$
\end_inset

.
 As in the tri-factorization above, 
\begin_inset Formula $L^{1}$
\end_inset

-norm approximations are used for what is essentially a 
\begin_inset Formula $L^{0}$
\end_inset

-norm optimization, and the sparsity term merely indicates the sum of all
 elements of each 
\begin_inset Formula $H$
\end_inset

, taken together and squared.
 The last line is an optional term used for 
\begin_inset Quotes eld
\end_inset

learning
\begin_inset Quotes erd
\end_inset

 the gene-image interaction matrix 
\begin_inset Formula $B$
\end_inset

, and its use is described below.
\end_layout

\begin_layout Standard
In order to determine the effect of including the gene-image interaction
 matrix 
\begin_inset Formula $B$
\end_inset

 in the optimization, we calculated 
\begin_inset Formula $W$
\end_inset

, 
\begin_inset Formula $H_{I}$
\end_inset

 and 
\begin_inset Formula $H_{G}$
\end_inset

 under three conditions:
\end_layout

\begin_layout Enumerate
Omit the interaction matrix 
\begin_inset Formula $B$
\end_inset

 altogether.
 That is, set 
\begin_inset Formula $\lambda_{2}=\lambda_{3}=0$
\end_inset

.
\end_layout

\begin_layout Enumerate
Set 
\begin_inset Formula $B$
\end_inset

 equal to the correlation matrix between columns of 
\begin_inset Formula $X_{I}$
\end_inset

 and 
\begin_inset Formula $X_{G}$
\end_inset

, namely 
\begin_inset Formula $B=B_{0}$
\end_inset

.
 Do not update 
\begin_inset Formula $B$
\end_inset

 and set 
\begin_inset Formula $\lambda_{3}=0$
\end_inset

.
\end_layout

\begin_layout Enumerate
Initialize 
\begin_inset Formula $B_{0}$
\end_inset

 as the correlation matrix between columns of 
\begin_inset Formula $X_{I}$
\end_inset

 and 
\begin_inset Formula $X_{G}$
\end_inset

.
 During each iteration, set 
\begin_inset Formula $B=H_{I}^{T}H_{G}$
\end_inset

, and set 
\begin_inset Formula $\lambda_{3}>0$
\end_inset

 in order to keep 
\begin_inset Formula $B$
\end_inset

 somewhat constrained by its initial value.
 This adds an additional constraint on 
\begin_inset Formula $H_{I}$
\end_inset

 and 
\begin_inset Formula $H_{G}$
\end_inset

, encouraging them to develop coefficients that incorporate correlated gene
 expression and image features.
 
\end_layout

\begin_layout Standard
We also wish to determine whether finding a common basis of 
\begin_inset Formula $X_{G}$
\end_inset

 and 
\begin_inset Formula $X_{I}$
\end_inset

 is a better predictor than simply computing a basis for 
\begin_inset Formula $X_{G}$
\end_inset

 or 
\begin_inset Formula $X_{I}$
\end_inset

 separately.
 In order to do this, we also compute a simple bi-factorization to minimize
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathcal{F}(W,H)=||X-WH||_{F}^{2}$
\end_inset

 for 
\begin_inset Formula $X=X_{I}$
\end_inset

 and 
\begin_inset Formula $X=X_{G}$
\end_inset

 separately.
\end_layout

\begin_layout Standard
As in the tri-factorization above, 
\begin_inset Formula $\mathcal{F}$
\end_inset

 is not a convex function.
 In order to force convexity we fix two of 
\begin_inset Formula $W$
\end_inset

, 
\begin_inset Formula $H_{I}$
\end_inset

 and 
\begin_inset Formula $H_{G}$
\end_inset

, compute the remaining variable, and repeat, updating each in turn.
 Determining the appropriate multiplicative update rules for 
\begin_inset Formula $W$
\end_inset

,
\begin_inset Formula $H_{I}$
\end_inset

 and 
\begin_inset Formula $H_{G}$
\end_inset

 proceeeds as follows, following the supplementary matrial in 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang:Bioinformatics:2011"

\end_inset

.
 First, the objective function 
\begin_inset Formula $\mathcal{F}$
\end_inset

 can be rewritten as the following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mathcal{F} & =Tr(X_{G}X_{G}^{T})-2Tr(X_{G}H_{G}^{T}W^{T})+Tr(WH_{G}H_{G}^{T}W^{T})\\
 & +Tr(X_{I}X_{I}^{T})-2Tr(X_{I}H_{I}^{T}W^{T})+Tr(WH_{I}H_{I}^{T}W^{T})\\
 & -\lambda_{1}Tr(H_{G}AH_{G}^{T})-\lambda_{2}Tr(H_{I}BH_{G}^{T})+\gamma_{1}Tr(WW^{T})\\
 & +\gamma_{2}(e_{1\times k}H_{I}H_{I}^{T}e_{k\times1}^{T}+e_{1\times k}H_{G}H_{G}^{T}e_{k\times1}^{T})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
To minimize 
\begin_inset Formula $\mathcal{F}$
\end_inset

 according to the additional constraints 
\begin_inset Formula $W\geq0$
\end_inset

, 
\begin_inset Formula $H_{I}\geq0$
\end_inset

 and 
\begin_inset Formula $H_{G}\geq0$
\end_inset

 we create the Lagrangian 
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\[
\mathcal{L}(W,H_{I},H_{G})=\mathcal{F}+Tr(\Phi W^{T})+Tr(\Psi_{I}H_{I}^{T})+Tr(\Psi_{G}H_{G}^{T})
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\Phi=[\phi_{ij}]$
\end_inset

, 
\begin_inset Formula $\Psi_{I}=[(\psi_{I})_{ij}]$
\end_inset

 and 
\begin_inset Formula $\Psi_{G}=[(\psi_{G})_{ij}]$
\end_inset

 are the Lagrange multipliers for the constraints 
\begin_inset Formula $W_{ij}\geq0$
\end_inset

,
\begin_inset Formula $(H_{I})_{ij}\geq0$
\end_inset

 and 
\begin_inset Formula $(H_{G})_{ij}\geq0$
\end_inset

 respectively.
 The partial derivatives of 
\begin_inset Formula $\mathcal{L}$
\end_inset

 with respect to 
\begin_inset Formula $W$
\end_inset

, 
\begin_inset Formula $H_{I}$
\end_inset

 and 
\begin_inset Formula $H_{G}$
\end_inset

 are:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial\mathcal{L}}{\partial W} & =-2X_{I}H_{I}^{T}+2WH_{I}H_{I}^{T}-2X_{G}X_{G}^{T}+2WH_{G}H_{G}^{T}+2\gamma_{1}W+\Phi\\
\frac{\partial\mathcal{L}}{\partial H_{I}} & =-2W^{T}X_{I}+2W^{T}WH_{I}-\lambda_{2}H_{G}B^{T}+\gamma_{2}2e_{k\times k}H_{I}-\lambda_{3}2H_{G}B^{T}+\lambda_{3}2H_{G}H_{G}^{T}H_{I}+\Psi_{I}\\
\frac{\partial\mathcal{L}}{\partial H_{G}} & =-2W^{T}X_{G}+2W^{T}WH_{G}-\lambda_{1}2H_{G}A-\lambda2H_{I}B+\gamma_{2}2e_{k\times k}H_{G}-\lambda_{3}2H_{i}B_{0}+2H_{I}H_{I}^{T}H_{G}+\Psi_{G}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Using the KKT condition 
\begin_inset Formula $\phi_{i}{}_{j}W_{i}{}_{j}=0$
\end_inset

, 
\begin_inset Formula $(\psi_{I})_{i}{}_{j}(H_{I})_{i}{}_{j}=0$
\end_inset

 and 
\begin_inset Formula $(\psi_{G})_{i}{}_{j}(H_{G})_{i}{}_{j}=0$
\end_inset

, we can set each partial derivative to 0 and write
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
0= & (-2X_{I}H_{I}^{T}-2X_{G}X_{G}^{T})_{i}{}_{j}W_{i}{}_{j}+(2WH_{I}H_{I}^{T}+2WH_{G}H_{G}^{T}+2\gamma_{1}W)_{i}{}_{j}W_{i}{}_{j}\\
0= & (-2W^{T}X_{I}-\lambda_{2}H_{G}B^{T}-\lambda_{3}2H_{G}B^{T})_{i}{}_{j}(H_{I})_{i}{}_{j}+(2W^{T}WH_{I}+\gamma_{2}2e_{k\times k}H_{I}+\lambda_{3}2H_{G}H_{G}^{T}H_{I})_{i}{}_{j}(H_{I})_{i}{}_{j}\\
0= & (-2W^{T}X_{G}-\lambda_{1}2H_{G}A-\lambda2H_{I}B-\lambda_{3}2H_{i}B_{0})_{i}{}_{j}(H_{G})_{i}{}_{j}+(2W^{T}WH_{G}+\gamma_{2}2e_{k\times k}H_{G}+2H_{I}H_{I}^{T}H_{G})_{i}{}_{j}(H_{G})_{i}{}_{j}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Moving the first term of each to the right side of the equation and dividing
 gives the following update rules:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
W_{i}{}_{j}\leftarrow\frac{(X_{I}H_{I}^{T}+X_{G}X_{G}^{T})_{ij}}{(WH_{I}H_{I}^{T}+WH_{G}H_{G}^{T}+\gamma_{1}W)_{ij}}
\]

\end_inset


\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\[
(H_{I})_{ij}\leftarrow\frac{(W^{T}X_{I}+\frac{\lambda_{2}}{2}H_{G}B^{T}+\lambda_{3}H_{G}B_{0}^{T})_{ij}}{(W^{T}WH_{I}+\gamma_{2}e_{k\times k}H_{I}+\lambda_{3}H_{G}H_{G}^{T}H_{I})_{ij}}
\]

\end_inset


\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\[
(H_{G})_{ij}\leftarrow\frac{(W^{T}X_{G}+\lambda_{1}H_{G}A+\frac{\lambda_{2}}{2}H_{I}B+\lambda_{3}H_{I}B_{0})_{ij}}{(W^{T}WH_{G}+\gamma_{2}e_{k\times k}H_{G}+\lambda_{3}H_{I}H_{I}^{T}H_{G})_{ij}}
\]

\end_inset


\end_layout

\begin_layout Standard
As with the trifactorization multiplicative updates, a small epsilon is
 added to the demoninator of each update to prevent devision by zero.
 
\end_layout

\begin_layout Standard
Following 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang:Bioinformatics:2011"

\end_inset

, parameters 
\begin_inset Formula $\lambda_{1}$
\end_inset

, 
\begin_inset Formula $\lambda_{2}$
\end_inset

, 
\begin_inset Formula $\lambda_{3}$
\end_inset

, 
\begin_inset Formula $\gamma_{1}$
\end_inset

 and 
\begin_inset Formula $\gamma_{2}$
\end_inset

 were set to 0.0001, 0.01, 0.001, 20 and 10, respectively.
 A parameter sweep confirmed that these values give consistently reasonable
 results.
 20 factorizations were performed for each of the 5 conditions detailed
 above (3 complete bi-factorizations, one image-only and one gene-only).
 For each optimization, 
\begin_inset Formula $W$
\end_inset

, 
\begin_inset Formula $H_{I}$
\end_inset

 and 
\begin_inset Formula $H_{G}$
\end_inset

 were initialized to random values between 0 and 1, and the same 
\begin_inset Formula $W$
\end_inset

, 
\begin_inset Formula $H_{I}$
\end_inset

 and 
\begin_inset Formula $H_{G}$
\end_inset

 were used for each of the 5 methods.
 Multiplicative updates were repeated until the objective function was improved
 by less than 0.01%.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:convergence-trajectories"

\end_inset

 shows typical trajectories of the objective functions for each method.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/gene-image-bi-fact-convergence.png
	width 3.5in

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Typical convergence trajectories of the objective function for five bi-factoriza
tion methods.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:convergence-trajectories"

\end_inset


\end_layout

\end_inset

For each initialization of 
\begin_inset Formula $W$
\end_inset

, 
\begin_inset Formula $H_{I}$
\end_inset

 and 
\begin_inset Formula $H_{G}$
\end_inset

, each of the optimization algorithms was run.
 Once 
\begin_inset Formula $W$
\end_inset

 was computed, subjects were ranked according to corresponding values of
 each column of 
\begin_inset Formula $W$
\end_inset

 in turn.
 The top 50 subjects were labeled Group 1 and the bottom 50 subjects were
 labeled Group 2.
 The log-rank test was then performed to determine whether the survival
 outcome of the two groups was significantly different.
 The number of columns of 
\begin_inset Formula $W$
\end_inset

 providing a significant separation of group outcomes was then counted.
\end_layout

\begin_layout Standard
Though this method is used in the literature for uncovering latent explanatory
 variables (see 
\begin_inset CommandInset citation
LatexCommand cite
key "Kim:20123RdInternationalWorkshopOnCognitiveInformation:2012"

\end_inset

), it is worth asking whether the count of columns leading to significant
 group difference has meaning.
 Non-negative matrix factorization does not guarantee orthogonal columns
 in its result.
 Columns of 
\begin_inset Formula $W$
\end_inset

 may be correlated, in which case counting 2 columns of 
\begin_inset Formula $W$
\end_inset

 that produce significant group differences may amount to tallying two vectors
 in a similar direction.
 As can be seen in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:W-correlations"

\end_inset

, the majority of columns of 
\begin_inset Formula $W$
\end_inset

 are nearly orthogonal.
 There are notable exceptions, however.
 In this example, column 24 is highly positively correlated with columns
 7 and 20, and there are a number of high negative correlations.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/gene-image-bi-fact-W-column-correlation.png
	width 3.5in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Example correlations between columns of W.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:W-correlations"

\end_inset


\end_layout

\end_inset

Due to the lack of an orthogonality constraint, columns of 
\begin_inset Formula $W$
\end_inset

 are not independent and column count cannot be used as a standalone measure
 of the ability of bi-factorization to uncover features that significantly
 differentiate patient groups.
 In order to provide a better basis of comparison, we created a randomized
 dataset and performed the same five factorization routines.
 We shuffled feature values across subjects within each feature (i.e.
 reordered each value in columns of 
\begin_inset Formula $X_{G}$
\end_inset

 and 
\begin_inset Formula $X_{I}$
\end_inset

) so that distributions of each feature were maintained.
 Results for both real and randomized data are shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:bi-factorization-results"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/bi-factorization-results.png
	width 4in

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Number of columns of 
\begin_inset Formula $W$
\end_inset

 that significantly differented patients into low- and high-survival groups
 at an 
\begin_inset Formula $p=0.05$
\end_inset

 significance level.
 Entries 1-3 are the 3 variations of the bifactorizaiton method.
 Entry 4 uses only gene data, and entry 5 uses only image data.
 Entries 6-10 are results from the same methods using shuffled data.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:bi-factorization-results"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The results we found were somewhat surprising.
 There was virtually no difference between variations of the bifactorization
 method, that is, including a gene-image interaction term did not impact
 the number of columns of 
\begin_inset Formula $W$
\end_inset

 significantly differentiating patients.
 However, including both types of data constrained the column count considerably
, as entries 1-3 have much less variation than entries 4 and 5 (gene-only
 and image-only, respectively) in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:bi-factorization-results"

\end_inset

.
 Another interesting result is that factorizations using real data consistently
 uncovered several columns predictive of patient survival, whereas the median
 number of columns from randomly permuted data predicting patient survival
 was much lower (0 in the case of the 3 methods using both data types).
\end_layout

\begin_layout Section
Clustering
\end_layout

\begin_layout Standard
Clustering is a general famliy of unsupervised learning methods for partitioning
 a data set into groups of similar observations.
 There are many varieties of clustering algorithms, and each is most usefully
 applied to data with particular characteristics.
 Classical clustering methods include heirarchical clustering and k-means
 clustering.
 Heirarchical clustering assigns group labels based on some distance measure
 between observations, grouping nearby observations together.
 The k-means method is similar in that it uses a distance measure between
 observations, but rather than using all pairwise distances between observations
 this method tries to optimally locate some number, 
\begin_inset Formula $k$
\end_inset

, of cluster centers and assign each observation to the nearest cluster
 center.
 
\end_layout

\begin_layout Standard
Because these methods are based on a distance measurement they may not be
 good choices for data with irregularly shaped clusters.
 High-dimensional data is also challenging for the classical clustering
 methods since variance of distance between observations approaches zero
 as dimension increases and as the number of observations decreases.
 We use a relatively modern, generally robust method called spectral clustering
 to evaluate the feature selection results provided by non-negative matrix
 trifactorization.
 Spectral clustering methods are able to operate on a more general notion
 of similarity between observations than k-means and hierarchical clustering,
 although choosing an appropriate similarity measure is important to arriving
 at reasonable results.
 This allows for selection of similarity measures that may be more appropriate
 than euclidean distance or some related distance.
 Additionally, spectral clustering methods have simlple implementations
 that take advantage of general-purpose linear algebraic operations.
\end_layout

\begin_layout Standard
Classification by spectral clustering on all features of the Wisconsin and
 Gene-Image data establishes a useful point of comparison for the feature-select
ion performance of the trifactorization, although we certainly expect trifactori
zation to perform better because it is a supervised method.
 
\end_layout

\begin_layout Subsection
Shared Nearest Neighbor Similarity
\end_layout

\begin_layout Standard
Some clustering algorithms operate in feature-space, whereas others require
 a graph-based representation of the data in the form of an adjacency matrix.
 Consider each observation as a node in a graph.
 An adjacency matrix 
\begin_inset Formula $A$
\end_inset

 consists of entries 
\begin_inset Formula $A_{i}{}_{j}$
\end_inset

 that correspond to some measure of 
\begin_inset Quotes eld
\end_inset

similarity
\begin_inset Quotes erd
\end_inset

 between observations/nodes 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

 in the graph.
 In order to construct 
\begin_inset Formula $A$
\end_inset

, we must use a method that creates a similarity measure between observations
 using their location in feature-space.
 In low-dimensional space, Euclidean distance is often used as a similarity
 measure.
 As the number of dimensions grows, however, all pairwise distances become
 more alike and correspondingly less information-bearing regarding pairwise
 observation similarity.
 
\end_layout

\begin_layout Standard
One method for evaluating the similarity of points is to look at the collection
 of points surrounding each in feature space.
 The idea is that if two points have a similar feature 
\begin_inset Quotes eld
\end_inset

neighborhood,
\begin_inset Quotes erd
\end_inset

 they will then be similar to each other.
 Using several nearest neighbors makes the Euclidean distance metric more
 robust to high dimensionality.
 The adjacency matrix is then comprised of the number of 
\begin_inset Quotes eld
\end_inset

neareset neighbors
\begin_inset Quotes erd
\end_inset

 two features have in common.
 The benefit of the SNN method is that it is relative to the local geometry.
 That is, SNN constructs a definition of similarity that is relatively robust
 in the face of high dimensionality and variations in density throughout
 the feature space.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/SNN graphic (CITE).png
	width 3.5in

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Creation of a Shared-Nearest-Neighbor (SNN) similarity matrix.
 (CITE) 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In order to construct a shared nearest neighbor adjacency matrix for subjects,
 pairwise Euclidean distance is first computed using standardized features.
 For each subject, distances to other subjects are ranked and the closest
 
\family typewriter

\begin_inset Formula $p$
\end_inset


\family default
 subjects to that subject are identified.
 Once each subject has a set of 
\begin_inset Formula $p$
\end_inset

 
\begin_inset Quotes eld
\end_inset

nearest neighbors,
\begin_inset Quotes erd
\end_inset

 the SNN similarity measure between two subjects 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

, 
\begin_inset Formula $A_{ij}$
\end_inset

, is calculated as the cardinality of the pairwise intersections between
 two subjects' nearest neighbor sets.
 The parameter 
\begin_inset Formula $p$
\end_inset

 is user-specified and controls the sparsity of the adjacency matrix and
 was chosen to be 30 in our calculations.
\end_layout

\begin_layout Subsection
Spectral Clustering
\end_layout

\begin_layout Standard
Clustering is a very popular unsupervised learning technique for finding
 related groups in large data sets.
 Due to the requirement to reduce the dimensionality of the data, we found
 it convenient to convert the data from feature space into a graph-based
 representation.
 Spectral clustering is a robust and well-understood method for clustering
 nodes in a graph.
 Accordingly, we use it to partition the adjacency matrix in order to uncover
 patient groups.
 
\end_layout

\begin_layout Standard
The basic spectral clustering method is to treat the data set as a graph
 with observations as vertices and similarity between observations as edge
 weights, calculate the corresponding adjacency matrix and the related graph
 Laplacian matrix 
\begin_inset Formula $L$
\end_inset

 and cluster the elements of a subset of eigenvectors of 
\begin_inset Formula $L$
\end_inset

.
 Once a similarity measure 
\begin_inset Formula $\phi$
\end_inset

 is chosen the corresponding adjacency matrix is calculated where each element
 
\begin_inset Formula $A_{ij}$
\end_inset

 is the similarity between observations 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
A_{ij} & =\phi(i,j)\label{eq:adjacency-matrix}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Next a graph Laplacian is calculated using the adjacency matrix 
\begin_inset Formula $A$
\end_inset

 and the diagonal matrix of row sums of 
\begin_inset Formula $A$
\end_inset

, called 
\begin_inset Formula $D$
\end_inset

.
 At this point another choice must be made between three graph Laplacians:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
L_{U} & =D-A\label{eq:unnormalized-graph-laplacian}\\
L_{sym} & =D^{-1/2}L_{U}D^{-1/2}\label{eq:normalized-graph-laplacian}\\
L_{rw} & =D^{-1}L_{U}\label{eq:random-walk-graph-laplacian}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
These graph Laplacian matrices are called, respectively, 'unnormalized',
 'normalized symmetric', and 'normalized random walk'.
 The tutorial 
\begin_inset CommandInset citation
LatexCommand cite
key "von2007tutorial"

\end_inset

 suggests using the distribution of degrees in the adjacency matrix as a
 deciding factor.
 If the distribution of degrees is narrow then the three graph Laplacians
 are likely to give similar clustering results.
 If the distribution of degrees is broad the normalized graph Laplacians
 may be better choices.
 Futhermore the normalized random walk graph Laplacian may be the better
 choice.
 
\begin_inset CommandInset citation
LatexCommand cite
key "von2007tutorial"

\end_inset

 gives several supporting arguments for this choice.
\end_layout

\begin_layout Standard
Finally, compute the 
\begin_inset Formula $k$
\end_inset

 smallest eigenvalues 
\begin_inset Formula $v_{n}$
\end_inset

 and corresponding eigenvectors 
\begin_inset Formula $u_{n}$
\end_inset

 of the chosed graph Laplacian, construct a matrix 
\begin_inset Formula $C$
\end_inset

 with columns 
\begin_inset Formula $u_{n}$
\end_inset

, and cluster the rows of 
\begin_inset Formula $C$
\end_inset

 (using a method such as k-means) to assign cluster labels.
 Since each row corresponds to an observation the row clusters of 
\begin_inset Formula $C$
\end_inset

 assign labels to the observations.
\end_layout

\begin_layout Standard
The distributions of SNN edge weights was generally broad for both the Wisconsin
 data and the gene-imge data.
 For this reason the random walk graph Laplacian was used for spectral clusterin
g in all cases.
\end_layout

\begin_layout Standard
Sparse graphs are typically easier to cluster.
 As shown in Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:wisconsin-edge-weights"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:gene-image-adjacency-feature-selection"

\end_inset

, selecting a subset of features leads to the construction of a much sparser
 graph, with the vast majority of pairwise edge weights equalling 0.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/wisconsin-SNN-edge-weight-distribution.png
	width 3in

\end_inset


\begin_inset Graphics
	filename figures/wisconsin-selected-features-SNN-edge-weight-distribution.png
	width 3in

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Edge weight distribution of the subject adjacency matrix 
\begin_inset Formula $A$
\end_inset

 with SNN similarity for Wisconsin data with and without feature selection.
 Note that feature selection creates a much sparser adjacency matrix.
 
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:wisconsin-edge-weights"

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/gene-image-SNN-edge-weight-distribution.png
	width 3in

\end_inset


\begin_inset Graphics
	filename figures/gene-image-selected-features-SNN-edge-weight-distribution.png
	width 3in

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Edge weight distribution of the adjacency matrix 
\begin_inset Formula $A$
\end_inset

 with SNN similarity for the Gene-Image dataset with and without feature
 selection.
 Note that once again the adjacency matrix is much sparser after using feature
 selection.
 
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:gene-image-adjacency-feature-selection"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Feature Selection
\end_layout

\begin_layout Standard
Spectral clustering is used here as a method for evaluating feature selection
 derived from matrix bifactorization, where subjects were labeled according
 to individual columns of 
\begin_inset Formula $F$
\end_inset

.
 The benefit of feature selection is evaluated with total cluster entropy,
 defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
H_{total} & =\sum_{i}\frac{|C_{i}|}{N}H_{C_{i}}\\
H_{C} & =-\sum\frac{|C_{i}^{j}|}{|C_{i}|}\log(\frac{|C_{i}^{j}|}{|C_{i}|})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $N$
\end_inset

 is the number of observations, 
\begin_inset Formula $i$
\end_inset

 indexes the clusters, and 
\begin_inset Formula $j$
\end_inset

 indexes the known subject labels.
 This formulation represents a sum of the entropy of individual clusters,
 weighted by cluster size.
\end_layout

\begin_layout Standard
The Wisconsin breast cancer data set has relatively small dimensionality
 and is well-separated in the Cell 2 data, as can be seen in a plot of the
 first two singular vectors 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:wisconsin-top-2-singular-vectors"

\end_inset

.
 Spectral clustering classifies this data with total cluster entropy 0.4430
 (error rate of 0.11) without feature selection.
 Selecting the best 6 features as determined by matrix trifactorization,
 spectral clustering classifies this data with total cluster entropy 0.2553
 (error rate 0.09).
\end_layout

\begin_layout Standard
The Gene-Image dataset is high-dimensional and has a relatively small number
 of subjects.
 Spectral clustering with all features gave poor classification results,
 with total cluster entropy 0.9259 (error rate 0.35).
 Selecting features based on the corresponding trifactorization improved
 the results, giving total cluster entropy 0.8426 (error rate 0.32).
\end_layout

\begin_layout Standard
In order to provide a 2D visualization with and without feature selection,
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:wisconsin-top-2-singular-vectors"

\end_inset

 contains plots of the Wisconsin dataset projected onto the first two singular
 vectors with all features and only the top features, respectively.
 Interestingly, despite an improvement in spectral clustering using the
 SNN-derived adjacency matrix, there is negligable difference between the
 data in top-2 singular vector space using all and selected features.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/wisconsin-2-singular-vectors.png
	width 3in

\end_inset


\begin_inset Graphics
	filename figures/wisconsin-selected-features-2-singular-vectors.png
	width 3in

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Wisconsin data projected onto the top-2 singular vector space, with and
 without feature selection.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:wisconsin-top-2-singular-vectors"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/gene-image-2-singular-vectors.png
	width 3in

\end_inset


\begin_inset Graphics
	filename figures/gene-no-image-selected-features-2-singular-vectors.png
	width 1.5in

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Gene-Image data points projected onto the first 2 singular vectors of the
 individual gene expression and image feature data.
 The left plot shows the projection using all features, and the right plot
 shows the same projection using only the top 100 features.
 None of the image features were among the top 100 features as determined
 by non-negative trifactorization feature selection.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
It seems clear from our results that non-negative matrix trifactorization
 is a useful tool for supervised feature selection.
 Spectral clustering performance improved in both the Wisconsin and Gene-Image
 dataset when feature selection was applied.
 This is not surprising, but serves as an additional validation step of
 the non-negative matrix trifactorization feature selection method.
\end_layout

\begin_layout Standard
It is interesting that in the feature selection of the Gene-Image dataset,
 all top 100 features are from genetic data and not image data.
 To look at it another way, gene expression data appears to be more predictive
 of whether cancer will metastasize than visual inspection of the cancer
 cells themselves.
 This suggests that genetic tests are a critical biomarker in determining
 patient prognosis.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status collapsed

\begin_layout Plain Layout
gene-image data, all features
\end_layout

\begin_layout Plain Layout
15 subjects have cluster label 1 and clinical label -1 
\end_layout

\begin_layout Plain Layout
40 subjects have cluster label 1 and clinical label 1
\end_layout

\begin_layout Plain Layout
6 subjects have cluster label 2 and clinical label -1 
\end_layout

\begin_layout Plain Layout
10 subjects have cluster label 2 and clinical label 1 
\end_layout

\begin_layout Plain Layout
4 subjects have cluster label 3 and clinical label -1 
\end_layout

\begin_layout Plain Layout
6 subjects have cluster label 3 and clinical label 1 
\end_layout

\begin_layout Plain Layout
61 subjects have cluster label 4 and clinical label -1 
\end_layout

\begin_layout Plain Layout
106 subjects have cluster label 4 and clinical label 1
\end_layout

\begin_layout Plain Layout
total_entropy =
\end_layout

\begin_layout Plain Layout
0.9259
\end_layout

\begin_layout Plain Layout
cluster entropy: 
\end_layout

\begin_layout Plain Layout
cluster_entropy =
\end_layout

\begin_layout Plain Layout
0.8454 0.9544 0.9710 0.9470 
\end_layout

\begin_layout Plain Layout
gene-image data, top 100 features
\end_layout

\begin_layout Plain Layout
9 subjects have cluster label 1 and clinical label -1 
\end_layout

\begin_layout Plain Layout
14 subjects have cluster label 1 and clinical label 1 
\end_layout

\begin_layout Plain Layout
4 subjects have cluster label 2 and clinical label -1
\end_layout

\begin_layout Plain Layout
46 subjects have cluster label 2 and clinical label 1 
\end_layout

\begin_layout Plain Layout
29 subjects have cluster label 3 and clinical label -1 
\end_layout

\begin_layout Plain Layout
22 subjects have cluster label 3 and clinical label 1 
\end_layout

\begin_layout Plain Layout
44 subjects have cluster label 4 and clinical label -1 
\end_layout

\begin_layout Plain Layout
80 subjects have cluster label 4 and clinical label 1
\end_layout

\begin_layout Plain Layout
total_entropy =
\end_layout

\begin_layout Plain Layout
0.8426
\end_layout

\begin_layout Plain Layout
cluster entropy: 
\end_layout

\begin_layout Plain Layout
cluster_entropy =
\end_layout

\begin_layout Plain Layout
0.9656 0.4022 0.9864 0.9383 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Comment
status collapsed

\begin_layout Plain Layout
all features 
\end_layout

\begin_layout Plain Layout
Wisconsin clustering results, 10 noise features, 8 clusters
\end_layout

\begin_layout Plain Layout
92 subjects have cluster label 1 and clinical label -1 
\end_layout

\begin_layout Plain Layout
28 subjects have cluster label 1 and clinical label 1 
\end_layout

\begin_layout Plain Layout
3 subjects have cluster label 2 and clinical label -1 
\end_layout

\begin_layout Plain Layout
45 subjects have cluster label 2 and clinical label 1 
\end_layout

\begin_layout Plain Layout
118 subjects have cluster label 3 and clinical label -1 
\end_layout

\begin_layout Plain Layout
19 subjects have cluster label 3 and clinical label 1 
\end_layout

\begin_layout Plain Layout
0 subjects have cluster label 4 and clinical label -1 
\end_layout

\begin_layout Plain Layout
30 subjects have cluster label 4 and clinical label 1 
\end_layout

\begin_layout Plain Layout
98 subjects have cluster label 5 and clinical label -1 
\end_layout

\begin_layout Plain Layout
11 subjects have cluster label 5 and clinical label 1 
\end_layout

\begin_layout Plain Layout
0 subjects have cluster label 6 and clinical label -1 
\end_layout

\begin_layout Plain Layout
76 subjects have cluster label 6 and clinical label 1 
\end_layout

\begin_layout Plain Layout
12 subjects have cluster label 7 and clinical label -1 
\end_layout

\begin_layout Plain Layout
3 subjects have cluster label 7 and clinical label 1 
\end_layout

\begin_layout Plain Layout
34 subjects have cluster label 8 and clinical label -1 
\end_layout

\begin_layout Plain Layout
0 subjects have cluster label 8 and clinical label 1
\end_layout

\begin_layout Plain Layout
total_entropy =
\end_layout

\begin_layout Plain Layout
0.4430
\end_layout

\begin_layout Plain Layout
cluster entropy: 
\end_layout

\begin_layout Plain Layout
cluster_entropy =
\end_layout

\begin_layout Plain Layout
0.7838 
\end_layout

\begin_layout Plain Layout
0.3373 
\end_layout

\begin_layout Plain Layout
0.5808 
\end_layout

\begin_layout Plain Layout
0 
\end_layout

\begin_layout Plain Layout
0.4719 
\end_layout

\begin_layout Plain Layout
0 
\end_layout

\begin_layout Plain Layout
0.7219 
\end_layout

\begin_layout Plain Layout
0 
\end_layout

\begin_layout Plain Layout
selected features 
\end_layout

\begin_layout Plain Layout
Wisconsin clustering results, 10 noise features, 8 clusters
\end_layout

\begin_layout Plain Layout
55 subjects have cluster label 1 and clinical label -1 
\end_layout

\begin_layout Plain Layout
1 subjects have cluster label 1 and clinical label 1 
\end_layout

\begin_layout Plain Layout
0 subjects have cluster label 2 and clinical label -1 
\end_layout

\begin_layout Plain Layout
84 subjects have cluster label 2 and clinical label 1 
\end_layout

\begin_layout Plain Layout
38 subjects have cluster label 3 and clinical label -1 
\end_layout

\begin_layout Plain Layout
44 subjects have cluster label 3 and clinical label 1 
\end_layout

\begin_layout Plain Layout
73 subjects have cluster label 4 and clinical label -1
\end_layout

\begin_layout Plain Layout
0 subjects have cluster label 4 and clinical label 1 
\end_layout

\begin_layout Plain Layout
0 subjects have cluster label 5 and clinical label -1 
\end_layout

\begin_layout Plain Layout
37 subjects have cluster label 5 and clinical label 1 
\end_layout

\begin_layout Plain Layout
7 subjects have cluster label 6 and clinical label -1 
\end_layout

\begin_layout Plain Layout
42 subjects have cluster label 6 and clinical label 1 
\end_layout

\begin_layout Plain Layout
53 subjects have cluster label 7 and clinical label -1 
\end_layout

\begin_layout Plain Layout
2 subjects have cluster label 7 and clinical label 1 
\end_layout

\begin_layout Plain Layout
131 subjects have cluster label 8 and clinical label -1 
\end_layout

\begin_layout Plain Layout
2 subjects have cluster label 8 and clinical label 1
\end_layout

\begin_layout Plain Layout
total_entropy = 0.2553
\end_layout

\begin_layout Plain Layout
cluster entropy: 
\end_layout

\begin_layout Plain Layout
cluster_entropy =
\end_layout

\begin_layout Plain Layout
0.1292 0 0.9961 0 0 0.5917 0.2254 0.1126
\end_layout

\end_inset


\end_layout

\begin_layout Part
Discussion and Future Work
\end_layout

\begin_layout Standard
Non-negative trifactorization appeared to be a useful method for supervised
 feature selection.
 Given that predictive features are predicted by the ratio of rows of 
\begin_inset Formula $SG^{T}$
\end_inset

, bifactorization would be an equivalent, simpler method.
 The trifactorization method used here (and feature information gain, which
 gives similar results) only select features as they independently contribute
 to patient labels.
 Including a term with known feature interactions into the trifactorization,
 much as we did in the later bifactorization method, might provide better
 results.
 It is well known that information does not always optimally select features
 for use in classification, for example in situations where only a combination
 of features is predictive of class, but individual features are not.
 It would be worthwhile to test whether trifactorization with an additional
 interaction was useful in such cases.
 It is possible that rather than providing comparable results to information
 gain, feature selection might be considerably improved.
 
\end_layout

\begin_layout Standard
The findings presented in the bifactorization section are an interesting
 combination of negative and positive results.
 It is notable that including a gene-image interaction term did not appear
 to change the quality of the results at all.
 This may be a result of the parameter size used, and a parameter sweep
 of 
\begin_inset Formula $\lambda_{3}$
\end_inset

 would help determine this.
 It appears encouraging that using real data separates patient groups better
 than randomized data.
 Upon further reflection, however, we believe that the count of features
 separating patients into groups with different survival outcomes is not
 important without an orthogonality constraint on W.
 The presence of a single predictive column, combined with several additional
 correlated columns, would be enough to artifically raise the count.
 In addition, the entry-shuffled matrix used as a control removed the predictive
ness of the columns of W.
 However, the result could also be explained by the shuffling simply removing
 correlations between columns of X and accordingly lowering the count of
 predictive columns.
 Performing multiplicative updates with an orthogonality constraint is not
 difficult (see 
\begin_inset CommandInset citation
LatexCommand cite
key "Ding:OrthogonalNonnegativeMatrixTFactorizationsForClustering:2006"

\end_inset

 for examples), so we would include this requirement were we to perform
 this analysis in the future.
\end_layout

\begin_layout Standard
It would be convenient to have a way to uncover predictive basis vectors
 that determine survival outcome without knowing the survival rate beforehand.
 Perhaps using a cross-validation approach, combined with working backwards
 from basis vectors to combinations of important features, would be one
 way to build a model of feature predictiveness.
 
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "Sente References"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
